{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9757dcc0",
   "metadata": {},
   "source": [
    "# 1. Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755748a4",
   "metadata": {},
   "source": [
    "#### In this assignment, you are expected to build a model that classifies given news article in Azerbaijani into one of the pre-defined six categories. You will need to train your model on a dataset of 50000 news examples.\n",
    "#### You will need to read the data from data file (news.xlsx). It contains three columns: news category, news title, and the news content. Split the data into train and test sets. Reserve 20% of the data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a8cfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>News_Article</th>\n",
       "      <th>CategoryId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maraqlı</td>\n",
       "      <td>Naviforce Sport Saat 2016 ilə zövqlərin ahəngi</td>\n",
       "      <td>Naviforce Sport Saat 2016 Yapon Mexanizmi Yapo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maraqlı</td>\n",
       "      <td>Sinir ,oynaq , sinir bel ağrılarına 3 gündə son !</td>\n",
       "      <td>ŞOK ! ŞOK ! ŞOK ! Xanımlar və bəylər , bel və ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maraqlı</td>\n",
       "      <td>Dəyərindən qat-qat aşağı qiymətə Mənzil</td>\n",
       "      <td>Dəyərindən qat-qat Aşağı Qiymətə. Həzi Aslanov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>İdman</td>\n",
       "      <td>2024 və 2028-ci il olimpiadalarının keçiriləcə...</td>\n",
       "      <td>2028-ci il Yay Olimpiya və Paralimpiya Oyunla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dünya</td>\n",
       "      <td>Türkiyədə zəlzələ</td>\n",
       "      <td>Türkiyədə daha bir zəlzələ meydana gəlib.   L...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>İdman</td>\n",
       "      <td>Yay Olimpiadası-2024-ün ev sahibi bəlli oldu</td>\n",
       "      <td>2024-cü il Yay Olimpiya Oyunlarının Parisdə k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dünya</td>\n",
       "      <td>ABŞ-ın Rusiyadakı səfirliyi viza verilməsi üzr...</td>\n",
       "      <td>Moskvanın ABŞ səfirliyi ştatının azaldılması ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Siyasət</td>\n",
       "      <td>İlan Syalom: \"Dağlıq Qarabağ münaqişəsinin yen...</td>\n",
       "      <td>\"İrəvan yaxınlığında yerləşən Metsamor nüvə s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dünya</td>\n",
       "      <td>Yunanıstanda 5.1 bal gücündə zəlzələ oldu</td>\n",
       "      <td>Aralıq dənizində 5.1 bal gücündə zəlzələ baş ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dünya</td>\n",
       "      <td>Mərakeşli sərnişinin mədəsindən 102 kapsul nar...</td>\n",
       "      <td>Mərakeşdən Türkiyəyə gələn sərnişinin mədəsin...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Maraqlı</td>\n",
       "      <td>Məşhur müğənni səhnədə ayaq üstə dayana bilməd...</td>\n",
       "      <td>Müğənni Sərdar Ortaç məşhur otellərdən birind...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Maraqlı</td>\n",
       "      <td>Banklar üçün ziyan nə qədər olub?</td>\n",
       "      <td>\"Kasperski laboratoriyası\" antivirus şirkəti ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Maraqlı</td>\n",
       "      <td>Açıq məkandakı miniatürlər - FOTO</td>\n",
       "      <td>Art-instalyasiyalar ustası Ayzek Kordel Polşa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dünya</td>\n",
       "      <td>Corc Kluni suriyalı uşaqların məktəbinə 2,25 m...</td>\n",
       "      <td>Aktyor Corc Kluni və onun həyat yoldaşı Amali...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>İqtisadiyyat</td>\n",
       "      <td>Forbes: \"Bakıda olmuş hər bir kəs ora yenə qay...</td>\n",
       "      <td>ABŞ-ın Fransa və Böyük Britaniyada geniş oxuc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>İdman</td>\n",
       "      <td>Şenol Günəş Türkiyə millisinə “YOX” dedi</td>\n",
       "      <td>Şenol GünəşTürkiyə millisinin baş məşqçisi ol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Maraqlı</td>\n",
       "      <td>“GoPro QuikStories” videonu avtomatik ötürəcək...</td>\n",
       "      <td>\"GoPro\" şirkəti qısa videoların yaradılması v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>İdman</td>\n",
       "      <td>Tilsim qırıldı: “Qarabağ” pley-off mərhələsind...</td>\n",
       "      <td>23:00 Bu gün Çempionlar Liqasında III təsnifa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Dünya</td>\n",
       "      <td>Venesuelada vəziyyət daha da pisləşdi</td>\n",
       "      <td>Venesuelada aprelin 4-dən bu günədək davam ed...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Maraqlı</td>\n",
       "      <td>İki “Apple” qurğusunun istehsalı dayandırılır</td>\n",
       "      <td>ABŞ-ın \"Apple\" şirkəti \"iPod shuffle\" və \"iPo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category                                              Title  \\\n",
       "0        Maraqlı     Naviforce Sport Saat 2016 ilə zövqlərin ahəngi   \n",
       "1        Maraqlı  Sinir ,oynaq , sinir bel ağrılarına 3 gündə son !   \n",
       "2        Maraqlı            Dəyərindən qat-qat aşağı qiymətə Mənzil   \n",
       "3          İdman  2024 və 2028-ci il olimpiadalarının keçiriləcə...   \n",
       "4          Dünya                                 Türkiyədə zəlzələ    \n",
       "5          İdman      Yay Olimpiadası-2024-ün ev sahibi bəlli oldu    \n",
       "6          Dünya  ABŞ-ın Rusiyadakı səfirliyi viza verilməsi üzr...   \n",
       "7        Siyasət  İlan Syalom: \"Dağlıq Qarabağ münaqişəsinin yen...   \n",
       "8          Dünya         Yunanıstanda 5.1 bal gücündə zəlzələ oldu    \n",
       "9          Dünya  Mərakeşli sərnişinin mədəsindən 102 kapsul nar...   \n",
       "10       Maraqlı  Məşhur müğənni səhnədə ayaq üstə dayana bilməd...   \n",
       "11       Maraqlı                  Banklar üçün ziyan nə qədər olub?   \n",
       "12       Maraqlı                  Açıq məkandakı miniatürlər - FOTO   \n",
       "13         Dünya  Corc Kluni suriyalı uşaqların məktəbinə 2,25 m...   \n",
       "14  İqtisadiyyat  Forbes: \"Bakıda olmuş hər bir kəs ora yenə qay...   \n",
       "15         İdman          Şenol Günəş Türkiyə millisinə “YOX” dedi    \n",
       "16       Maraqlı  “GoPro QuikStories” videonu avtomatik ötürəcək...   \n",
       "17         İdman  Tilsim qırıldı: “Qarabağ” pley-off mərhələsind...   \n",
       "18         Dünya              Venesuelada vəziyyət daha da pisləşdi   \n",
       "19       Maraqlı      İki “Apple” qurğusunun istehsalı dayandırılır   \n",
       "\n",
       "                                         News_Article  CategoryId  \n",
       "0   Naviforce Sport Saat 2016 Yapon Mexanizmi Yapo...           0  \n",
       "1   ŞOK ! ŞOK ! ŞOK ! Xanımlar və bəylər , bel və ...           0  \n",
       "2   Dəyərindən qat-qat Aşağı Qiymətə. Həzi Aslanov...           0  \n",
       "3    2028-ci il Yay Olimpiya və Paralimpiya Oyunla...           1  \n",
       "4    Türkiyədə daha bir zəlzələ meydana gəlib.   L...           2  \n",
       "5    2024-cü il Yay Olimpiya Oyunlarının Parisdə k...           1  \n",
       "6    Moskvanın ABŞ səfirliyi ştatının azaldılması ...           2  \n",
       "7    \"İrəvan yaxınlığında yerləşən Metsamor nüvə s...           3  \n",
       "8    Aralıq dənizində 5.1 bal gücündə zəlzələ baş ...           2  \n",
       "9    Mərakeşdən Türkiyəyə gələn sərnişinin mədəsin...           2  \n",
       "10   Müğənni Sərdar Ortaç məşhur otellərdən birind...           0  \n",
       "11   \"Kasperski laboratoriyası\" antivirus şirkəti ...           0  \n",
       "12   Art-instalyasiyalar ustası Ayzek Kordel Polşa...           0  \n",
       "13   Aktyor Corc Kluni və onun həyat yoldaşı Amali...           2  \n",
       "14   ABŞ-ın Fransa və Böyük Britaniyada geniş oxuc...           4  \n",
       "15   Şenol GünəşTürkiyə millisinin baş məşqçisi ol...           1  \n",
       "16   \"GoPro\" şirkəti qısa videoların yaradılması v...           0  \n",
       "17   23:00 Bu gün Çempionlar Liqasında III təsnifa...           1  \n",
       "18   Venesuelada aprelin 4-dən bu günədək davam ed...           2  \n",
       "19   ABŞ-ın \"Apple\" şirkəti \"iPod shuffle\" və \"iPo...           0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_excel('news.xlsx')\n",
    "df['CategoryId'] = df['Category'].factorize()[0]\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8252f9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ŞOK   ŞOK   ŞOK   Xanımlar və bəylər   bel və boyun ağrılarına son   Sinir və oynaq oynaq ağrılarına son  Remmatik ağrılara son  Miqren və şiddətli bel ağrılarına son   Bu ağrılardan azad olmağa hazırsınız O zaman sizə HEMANI şirkətinin hər dərdə dəva olan HAZAL kremini təqdim edirik  Bildiyimiz kimi hər kəs bel boyun və oynaq ağrılarından əziyyət çəkir Bu ağrılara necə şəfa tapmaq olar  Artıq HEMANI şirkətinin kremi ilə baş ağrılarına remmatizma ağrılarına oynaq ağrılarına miqren ağrılarına son qoyacaqsınız Tam təbii bitkilərdən hazırlanan təbiətin möcüzəsi sayılan Hazal kremi həm müalicəvi  həm də ağrıkəsici bir kremdir Dünyada Loğman Həkim şirkətinin istehsalı olan bitkilərdən hazırlanan bu kremi milyonlarla insan istifadə etdi və xeyrini gördü  Yaşlı insanlar üçün istifadəsi rahat olsun deyə lasyon halından krem halına çevirdik  Kremin əsas özünəməxsusluğu ondan ibarətdir ki həm müalicəvi kremdir həm də yeni yaranan ağrıları həmən aradan qaldırır Artıq aslılıq yaradan ağrıkəsicilərə ehtiyyac qalmayacaq  Kremiağrı olan nahiyəyə massaj şəklində yayacaqsınız və 5 dəqiqə ərzində ağrılardan azad olacaqsınız 5 dəqiqə ərzində aradan qaldırmaqla yanaşı ağrı olan nahiyəni müalicə edir Tablet deyil kapsula deyil sadəcə istifadəsi çox rahat olan şəfalı bitki tərkibli dəva kremidir Bu krem yalnız yaşlılar üçün deyil hər kəs üçün nəzərdə tutulub Hər dərdə Haza kremi artıq Azərbaycanda  ƏLAQƏ ÜÇÜN 055 239 54 49 BURADAN SİFARİŞ ET var SC CId    150627  SC Domain  n ads3 adnow com  SC Start 150627  new Date  getTime    Sağlamlıq   '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def special_char(text):\n",
    "  reviews = ''\n",
    "  for x in text:\n",
    "    if x.isalnum():\n",
    "      reviews = reviews + x\n",
    "    else:\n",
    "      reviews = reviews + ' '\n",
    "  return reviews\n",
    "df['News_Article'] = df['News_Article'].apply(special_char)\n",
    "df.News_Article[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2af0fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'şok   şok   şok   xanımlar və bəylər   bel və boyun ağrılarına son   sinir və oynaq oynaq ağrılarına son  remmatik ağrılara son  miqren və şiddətli bel ağrılarına son   bu ağrılardan azad olmağa hazırsınız o zaman sizə hemani şirkətinin hər dərdə dəva olan hazal kremini təqdim edirik  bildiyimiz kimi hər kəs bel boyun və oynaq ağrılarından əziyyət çəkir bu ağrılara necə şəfa tapmaq olar  artıq hemani şirkətinin kremi ilə baş ağrılarına remmatizma ağrılarına oynaq ağrılarına miqren ağrılarına son qoyacaqsınız tam təbii bitkilərdən hazırlanan təbiətin möcüzəsi sayılan hazal kremi həm müalicəvi  həm də ağrıkəsici bir kremdir dünyada loğman həkim şirkətinin istehsalı olan bitkilərdən hazırlanan bu kremi milyonlarla insan istifadə etdi və xeyrini gördü  yaşlı insanlar üçün istifadəsi rahat olsun deyə lasyon halından krem halına çevirdik  kremin əsas özünəməxsusluğu ondan ibarətdir ki həm müalicəvi kremdir həm də yeni yaranan ağrıları həmən aradan qaldırır artıq aslılıq yaradan ağrıkəsicilərə ehtiyyac qalmayacaq  kremiağrı olan nahiyəyə massaj şəklində yayacaqsınız və 5 dəqiqə ərzində ağrılardan azad olacaqsınız 5 dəqiqə ərzində aradan qaldırmaqla yanaşı ağrı olan nahiyəni müalicə edir tablet deyil kapsula deyil sadəcə istifadəsi çox rahat olan şəfalı bitki tərkibli dəva kremidir bu krem yalnız yaşlılar üçün deyil hər kəs üçün nəzərdə tutulub hər dərdə haza kremi artıq azərbaycanda  əlaqə üçün 055 239 54 49 buradan si̇fari̇ş et var sc cid    150627  sc domain  n ads3 adnow com  sc start 150627  new date  gettime    sağlamlıq   '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_lower(text):\n",
    "   return text.lower()\n",
    "df['News_Article'] = df['News_Article'].apply(convert_lower)\n",
    "df['News_Article'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b944e1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "def remove_stopwords(text):\n",
    "  stop_words = {'üçün','hər','o','bu','hər','ilə','və','olan','kəs','da','də','qədər','kimi','belə','elə','isə','ki','əgər','çox'}\n",
    "  words = word_tokenize(text)\n",
    "  return [x for x in words if x not in stop_words]\n",
    "df['News_Article'] = df['News_Article'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e65fb9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['News_Article']\n",
    "y = df['CategoryId']\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf71c0",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018387df",
   "metadata": {},
   "source": [
    "#### You are expected to extract features from the news articles using bag-of-words and tf-idf representation techniques. You can use library tools (CountVectorizer, TfidfVectorizer) for this purpose. You may want to limit the vocabulary size by choosing most frequent 3000 tokens. Experiment with different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "becc7748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "['00', '000', '01', '04', '05', '08', '09', '10', '100', '1000', '11', '12', '120', '125', '13', '14', '15', '150', '150627', '16']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer(max_features=3000)\n",
    "X_train=tfidf.fit_transform(X_train)\n",
    "X_test=tfidf.transform(X_test)\n",
    "print(len(tfidf.get_feature_names()))\n",
    "print(tfidf.get_feature_names()[:20])\n",
    "\n",
    "train_arr=X_train.toarray()\n",
    "test_arr=X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1112a349",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16968/1796335917.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategoryId\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNews_Article\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X.shape = \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y.shape = \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1203\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "X = np.array(df.News_Article.values)\n",
    "y = np.array(df.CategoryId.values)\n",
    "cv = CountVectorizer(max_features = 3000)\n",
    "X = cv.fit_transform(df.News_Article).toarray()\n",
    "print(\"X.shape = \",X.shape)\n",
    "print(\"y.shape = \",y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83629159",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01307852",
   "metadata": {},
   "source": [
    "#### For each of the a) logistic regression, b) naïve Bayes, c) stochastic gradient descent classifier, and d) random forest classifier:\n",
    "- Train it on the training data with Tf-idf representations\n",
    "- Test the model using test set\n",
    "- Achieve accuracy rate of ~80%\n",
    "- Evaluate precision, recall, and F1 scores\n",
    "- Construct confusion matrix\n",
    "#### You can use LogisticRegression, MultinomialNB, SGDClassifier, and RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0823044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import make_scorer, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "perform_list = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d69e79ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83.14"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X_train, y_train)\n",
    "y_pred = lgr.predict(X_test)\n",
    "a=round(accuracy_score(y_test,y_pred)*100,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec06ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_name, est_c, est_pnlty):\n",
    "\n",
    "    mdl=''\n",
    "\n",
    "    if model_name == 'Logistic Regression':\n",
    "\n",
    "        mdl = LogisticRegression()\n",
    "\n",
    "    elif model_name == 'Multinomial Naive Bayes':\n",
    "\n",
    "        mdl = MultinomialNB(alpha=1.0,fit_prior=True)\n",
    "\n",
    "    elif model_name == 'SGD Classifier':\n",
    "\n",
    "        mdl = SGDClassifier()\n",
    "\n",
    "    elif model_name == 'Random Forest':\n",
    "\n",
    "        mdl = RandomForestClassifier(n_estimators=100 ,criterion='entropy' , random_state=0)\n",
    "        \n",
    "        \n",
    "    oneVsRest = OneVsRestClassifier(mdl)\n",
    "\n",
    "    oneVsRest.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = oneVsRest.predict(X_test)\n",
    "    \n",
    "    accuracy = round(accuracy_score(y_test, y_pred) * 100, 2)\n",
    "\n",
    "    # Get precision, recall, f1 scores\n",
    "\n",
    "    precision, recall, f1score, support = score(y_test, y_pred, average='micro')\n",
    "\n",
    "    print(f'Test Accuracy Score of Basic {model_name}: % {accuracy}')\n",
    "\n",
    "    print(f'Precision : {precision}')\n",
    "\n",
    "    print(f'Recall : {recall}')\n",
    "\n",
    "    print(f'F1-score : {f1score}')\n",
    "\n",
    "    # Add performance parameters to list\n",
    "\n",
    "    perform_list.append(dict([\n",
    "\n",
    "    ('Model', model_name),\n",
    "\n",
    "    ('Test Accuracy', round(accuracy, 2)),\n",
    "\n",
    "    ('Precision', round(precision, 2)),\n",
    "\n",
    "    ('Recall', round(recall, 2)),\n",
    "\n",
    "    ('F1', round(f1score, 2))\n",
    "\n",
    "    ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77bfe719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score of Basic Logistic Regression: % 82.88\n",
      "Precision : 0.8288\n",
      "Recall : 0.8288\n",
      "F1-score : 0.8288\n"
     ]
    }
   ],
   "source": [
    "run_model('Logistic Regression', est_c=None, est_pnlty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e47ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score of Basic Multinomial Naive Bayes: % 77.92\n",
      "Precision : 0.7792\n",
      "Recall : 0.7792\n",
      "F1-score : 0.7792\n"
     ]
    }
   ],
   "source": [
    "run_model('Multinomial Naive Bayes', est_c=None, est_pnlty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebf6dc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score of Basic SGD Classifier: % 83.03\n",
      "Precision : 0.8303\n",
      "Recall : 0.8303\n",
      "F1-score : 0.8303\n"
     ]
    }
   ],
   "source": [
    "run_model('SGD Classifier', est_c=None, est_pnlty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e35b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score of Basic Random Forest: % 81.8\n",
      "Precision : 0.818\n",
      "Recall : 0.818\n",
      "F1-score : 0.818\n"
     ]
    }
   ],
   "source": [
    "run_model('Random Forest', est_c=None, est_pnlty=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c6704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
